{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/coconut/coconut_split.csv')\n",
    "df.rename(columns={'identifier': 'id', 'smiles': 'target', 'split': 'split'}, inplace=True)\n",
    "df = df[['id', 'target', 'split']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate equivalent absolute SMILES. This step is done here to remove faulty SMILES from the dataset. The step is repeated later for the dataset augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(smiles):\n",
    "    substitutions = {\n",
    "        r'\\[K[@,H]*\\]': '[K]',\n",
    "        r'\\[B[@,H]*\\]': 'B',\n",
    "        r'\\[Na[@,H,+,-]*\\]': '[Na]',\n",
    "        r'\\[C[@,H]*\\]': 'C',\n",
    "        r'\\[N[@,H]*\\]': 'N',\n",
    "        r'\\[O[@,H]*\\]': 'O',\n",
    "        r'\\[S[@,H]*\\]': 'S',\n",
    "        r'\\[P[@,H]*\\]': 'P',\n",
    "        r'\\[F[@,H]*\\]': 'F',\n",
    "        r'\\[Cl[@,H]*\\]': '[Cl]',\n",
    "        r'\\[Br[@,H]*\\]': '[Br]',\n",
    "        r'\\[I[@,H]*\\]': 'I',\n",
    "        r'@': '',\n",
    "        r'/': '',\n",
    "        r'\\\\': '',\n",
    "        r'\\[C\\]': 'C'\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in substitutions.items():\n",
    "        smiles = re.sub(pattern, replacement, smiles)\n",
    "\n",
    "    return smiles\n",
    "\n",
    "df['source'] = df['target'].apply(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(row):\n",
    "    target_mol = Chem.MolFromSmiles(row['target'])\n",
    "    source_mol = Chem.MolFromSmiles(row['source'])\n",
    "    target_smiles = Chem.MolToSmiles(target_mol, canonical=True, isomericSmiles=False)\n",
    "    source_smiles = Chem.MolToSmiles(source_mol, canonical=True, isomericSmiles=False)\n",
    "    return target_smiles == source_smiles\n",
    "\n",
    "checks = df.apply(sanity_check, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove annoying SMILES (where the two flat structures are not the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[checks].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dataset with scrambled stereocenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def scramble_stereochemistry(text):\n",
    "    def random_replacement(match):\n",
    "        return random.choice([\"@\", \"@@\"])\n",
    "    text = re.sub(r'@@|@', random_replacement, text)\n",
    "    text = text.replace('/', 'TEMP_SLASH').replace('\\\\', '/').replace('TEMP_SLASH', '\\\\')\n",
    "    return text\n",
    "\n",
    "scrambled_df = df.copy()\n",
    "scrambled_df['target'] = scrambled_df['target'].apply(scramble_stereochemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment data by SMILES randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare function to randomize SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_smiles(smiles, seed=None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    atoms = list(range(mol.GetNumAtoms()))\n",
    "    np.random.shuffle(atoms)\n",
    "    new_mol = Chem.RenumberAtoms(mol, atoms)\n",
    "    return Chem.MolToSmiles(new_mol, canonical=False, isomericSmiles=True)\n",
    "\n",
    "def augment_dataframe(df, factor):\n",
    "    augmented_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for i in range(factor):\n",
    "            new_smiles = randomize_smiles(row['target'], seed=i)\n",
    "            augmented_data.append({'id': row['id'], 'target': new_smiles, 'split': row['split']})\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets with different augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_2x = augment_dataframe(df, 2)\n",
    "augmented_5x = augment_dataframe(df, 5)\n",
    "scrambled_5x = augment_dataframe(scrambled_df, 5)\n",
    "augmented_10x = augment_dataframe(df, 10)\n",
    "augmented_20x = augment_dataframe(df, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate equivalent achiral SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_2x['source'] = augmented_2x['target'].apply(flatten)\n",
    "augmented_5x['source'] = augmented_5x['target'].apply(flatten)\n",
    "scrambled_5x['source'] = scrambled_5x['target'].apply(flatten)\n",
    "augmented_10x['source'] = augmented_10x['target'].apply(flatten)\n",
    "augmented_20x['source'] = augmented_20x['target'].apply(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_2x.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_5x.drop_duplicates(subset='target', inplace=True)\n",
    "scrambled_5x.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_10x.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_20x.drop_duplicates(subset='target', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_2x_shuffled = augmented_2x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "augmented_5x_shuffled = augmented_5x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "scrambled_5x_shuffled = scrambled_5x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "augmented_10x_shuffled = augmented_10x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "augmented_20x_shuffled = augmented_20x.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export augmented datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save augmented data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'source', 'target', 'split']].to_csv('data/dataset_not_augmented.csv', index=False)\n",
    "augmented_2x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_2x.csv', index=False)\n",
    "augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_5x.csv', index=False)\n",
    "scrambled_5x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_scrambled_5x.csv', index=False)\n",
    "augmented_10x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_10x.csv', index=False)\n",
    "augmented_20x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_20x.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiralpred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
