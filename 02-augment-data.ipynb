{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "from eval_functions import flatten  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility & Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_smiles(smiles):\n",
    "    \"\"\"Canonicalize SMILES (preserving aromaticity).\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return Chem.MolToSmiles(mol) if mol is not None else smiles\n",
    "\n",
    "def sanity_check(row):\n",
    "    \"\"\"Sanity-check that target and flattened source yield the same canonical SMILES.\"\"\"\n",
    "    target_mol = Chem.MolFromSmiles(row['target'])\n",
    "    source_mol = Chem.MolFromSmiles(row['source'])\n",
    "    if target_mol is None or source_mol is None:\n",
    "        return False\n",
    "    target_smiles = Chem.MolToSmiles(target_mol, canonical=True, isomericSmiles=False)\n",
    "    source_smiles = Chem.MolToSmiles(source_mol, canonical=True, isomericSmiles=False)\n",
    "    return target_smiles == source_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stereochemistry Modification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_swap_stereochemistry(text):\n",
    "    text = text.replace('@@', 'TEMP_DOUBLE_AT')\n",
    "    text = text.replace('@', '@@')\n",
    "    text = text.replace('TEMP_DOUBLE_AT', '@')\n",
    "    text = text.replace('/', 'TEMP_SLASH').replace('\\\\', '/').replace('TEMP_SLASH', '\\\\')\n",
    "    return text\n",
    "\n",
    "def scramble_stereochemistry(text):\n",
    "    def random_replacement(match):\n",
    "        return random.choice([\"@\", \"@@\"])\n",
    "    def random_slash_backslash(match):\n",
    "        return random.choice([\"/\", \"\\\\\"])\n",
    "    text = re.sub(r'@@|@', random_replacement, text)\n",
    "    text = re.sub(r'/|\\\\', random_slash_backslash, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMILES Randomization & Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_smiles(smiles, seed_val=None):\n",
    "    if seed_val is not None:\n",
    "        np.random.seed(seed_val)\n",
    "    else:\n",
    "        np.random.seed()\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return smiles\n",
    "    atoms = list(range(mol.GetNumAtoms()))\n",
    "    np.random.shuffle(atoms)\n",
    "    new_mol = Chem.RenumberAtoms(mol, atoms)\n",
    "    return Chem.MolToSmiles(new_mol, canonical=False, isomericSmiles=True)\n",
    "\n",
    "def randomize_augment_dataframe(df, factor):\n",
    "    \"\"\"Generate `factor` randomized SMILES per row.\"\"\"\n",
    "    augmented_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        for i in range(factor):\n",
    "            new_smiles = randomize_smiles(row['target'], seed_val=i)\n",
    "            augmented_data.append({\n",
    "                'id': row['id'],\n",
    "                'target': new_smiles,\n",
    "                'split': row['split']\n",
    "            })\n",
    "    return pd.DataFrame(augmented_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial Stereochemistry Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions = {\n",
    "    r'\\[K[@,H]*\\]': '[K]',\n",
    "    r'\\[B[@,H]*\\]': 'B',\n",
    "    r'\\[Na[@,H,+,-]*\\]': '[Na]',\n",
    "    r'\\[C[@,H]*\\]': 'C',\n",
    "    r'\\[N[@,H]*\\]': 'N',\n",
    "    r'\\[O[@,H]*\\]': 'O',\n",
    "    r'\\[S[@,H]*\\]': 'S',\n",
    "    r'\\[P[@,H]*\\]': 'P',\n",
    "    r'\\[F[@,H]*\\]': 'F',\n",
    "    r'\\[Cl[@,H]*\\]': '[Cl]',\n",
    "    r'\\[Br[@,H]*\\]': '[Br]',\n",
    "    r'\\[I[@,H]*\\]': 'I',\n",
    "    r'@': '',\n",
    "    r'/': '',\n",
    "    r'\\\\': '',\n",
    "    r'\\[C\\]': 'C'\n",
    "}\n",
    "\n",
    "def apply_substitutions(smiles):\n",
    "    for pattern, replacement in substitutions.items():\n",
    "        smiles = re.sub(pattern, replacement, smiles)\n",
    "    return smiles\n",
    "\n",
    "def generate_n_permutations(smiles, matches, num_to_remove, n, seed_val=None):\n",
    "    \"\"\"Generate exactly `n` permutations replacing `num_to_remove` matches.\"\"\"\n",
    "    permutations = set()\n",
    "    if seed_val is not None:\n",
    "        random.seed(seed_val)\n",
    "    if len(matches) < num_to_remove:\n",
    "        return list(permutations)\n",
    "    all_combinations = list(itertools.combinations(range(len(matches)), num_to_remove))\n",
    "    selected_combinations = all_combinations if len(all_combinations) <= n else random.sample(all_combinations, n)\n",
    "    for indices in selected_combinations:\n",
    "        modified_smiles = list(smiles)\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            match = matches[index]\n",
    "            match_start, match_end = match.start(), match.end()\n",
    "            match_str = smiles[match_start:match_end]\n",
    "            modified_smiles[match_start:match_end] = apply_substitutions(match_str)\n",
    "        permutations.add(''.join(modified_smiles))\n",
    "    return list(permutations)\n",
    "\n",
    "def generate_random_permutations(smiles, matches, max_augmentations=50, seed_val=None, max_attempts=1000):\n",
    "    if seed_val is not None:\n",
    "        random.seed(seed_val)\n",
    "    augmentations = set()\n",
    "    attempts = 0\n",
    "    while len(augmentations) < max_augmentations and attempts < max_attempts:\n",
    "        num_to_remove = random.randint(1, len(matches))\n",
    "        selected_indices = random.sample(range(len(matches)), num_to_remove)\n",
    "        modified_smiles = list(smiles)\n",
    "        for index in sorted(selected_indices, reverse=True):\n",
    "            match = matches[index]\n",
    "            match_start, match_end = match.start(), match.end()\n",
    "            match_str = smiles[match_start:match_end]\n",
    "            modified_smiles[match_start:match_end] = apply_substitutions(match_str)\n",
    "        augmentations.add(''.join(modified_smiles))\n",
    "        attempts += 1\n",
    "    return list(augmentations)\n",
    "\n",
    "def uniform_augment(smiles, n, seed_val=None):\n",
    "    \"\"\"Augment a SMILES string uniformly using substitutions.\"\"\"\n",
    "    pattern = r'(\\[.*?\\]|[\\\\/])'\n",
    "    matches = list(re.finditer(pattern, smiles))\n",
    "    if not matches:\n",
    "        return [smiles]\n",
    "    augmentations = set()\n",
    "    if len(matches) > 20:\n",
    "        augmentations.update(generate_random_permutations(smiles, matches, max_augmentations=50, seed_val=seed_val))\n",
    "    else:\n",
    "        for num_to_remove in range(1, len(matches) + 1):\n",
    "            augmentations.update(generate_n_permutations(smiles, matches, num_to_remove, n, seed_val))\n",
    "    return list(augmentations)\n",
    "\n",
    "def uniform_augment_dataframe(df, smiles_column, id_column, split_column, n=2, seed_val=None):\n",
    "    augmented_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        original_smiles = row[smiles_column]\n",
    "        augmented_smiles_list = uniform_augment(original_smiles, n, seed_val)\n",
    "        for augmented_smiles in augmented_smiles_list:\n",
    "            augmented_data.append({\n",
    "                id_column: row[id_column],\n",
    "                smiles_column: original_smiles,\n",
    "                'source': augmented_smiles,\n",
    "                split_column: row[split_column]\n",
    "            })\n",
    "    return pd.DataFrame(augmented_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing of One Input Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split_dataset(input_file, output_dir, aug_seed):\n",
    "    \"\"\"\n",
    "    Process one split-dataset (from a given seed) and generate all augmented versions.\n",
    "    \n",
    "    Parameters:\n",
    "      - input_file: Path to the original split CSV file.\n",
    "      - output_dir: Directory where output files will be saved.\n",
    "      - aug_seed: Seed used for augmentation (and scrambling) operations.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists.\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set global seeds for reproducibility\n",
    "    random.seed(aug_seed)\n",
    "    np.random.seed(aug_seed)\n",
    "    \n",
    "    # Read and prepare the dataset\n",
    "    df = pd.read_csv(input_file)\n",
    "    df.rename(columns={'identifier': 'id', 'smiles': 'target', 'split': 'split'}, inplace=True)\n",
    "    df = df[['id', 'target', 'split']].copy()\n",
    "    \n",
    "    # Canonicalize SMILES and generate the achiral (flattened) version\n",
    "    df['target'] = df['target'].apply(canonicalize_smiles)\n",
    "    df['source'] = df['target'].apply(flatten)\n",
    "    \n",
    "    # Apply sanity check and remove problematic rows\n",
    "    checks = df.apply(sanity_check, axis=1)\n",
    "    df = df[checks].reset_index(drop=True)\n",
    "    \n",
    "    # Create a scrambled version (scrambled stereochemistry)\n",
    "    scrambled_df = df.copy()\n",
    "    scrambled_df['target'] = scrambled_df['target'].apply(scramble_stereochemistry)\n",
    "    \n",
    "\n",
    "    # Augmentation: Randomized SMILES Generation\n",
    "    randomized = randomize_augment_dataframe(df, factor=1)\n",
    "    scrambled_randomized = randomize_augment_dataframe(scrambled_df, factor=1)\n",
    "    augmented_2x = randomize_augment_dataframe(df, factor=2)\n",
    "    augmented_5x = randomize_augment_dataframe(df, factor=5)\n",
    "    augmented_10x = randomize_augment_dataframe(df, factor=10)\n",
    "    augmented_20x = randomize_augment_dataframe(df, factor=20)\n",
    "    augmented_50x = randomize_augment_dataframe(df, factor=50)\n",
    "    \n",
    "    # Re-generate the achiral (flattened) SMILES for augmented datasets\n",
    "    for d in [randomized, scrambled_randomized, augmented_2x, augmented_5x, augmented_10x, augmented_20x, augmented_50x]:\n",
    "        d['source'] = d['target'].apply(flatten)\n",
    "    \n",
    "    # Partial Augmentations via Uniform Augmentation\n",
    "    partial_augmented_5x = uniform_augment_dataframe(df, 'target', 'id', 'split', n=5, seed_val=aug_seed)\n",
    "    randomized_partial_augmented_5x = uniform_augment_dataframe(randomized, 'target', 'id', 'split', n=5, seed_val=aug_seed)\n",
    "    scrambled_partial_augmented_5x = uniform_augment_dataframe(scrambled_df, 'target', 'id', 'split', n=5, seed_val=aug_seed)\n",
    "    randomized_scrambled_partial_augmented_5x = uniform_augment_dataframe(scrambled_randomized, 'target', 'id', 'split', n=5, seed_val=aug_seed)\n",
    "    mixed_augmented = uniform_augment_dataframe(augmented_10x, 'target', 'id', 'split', n=1, seed_val=aug_seed)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    for d in [randomized, augmented_2x, augmented_5x, augmented_10x, augmented_20x, augmented_50x]:\n",
    "        d.drop_duplicates(subset='target', inplace=True)\n",
    "    for d in [partial_augmented_5x, randomized_partial_augmented_5x,\n",
    "              scrambled_partial_augmented_5x, randomized_scrambled_partial_augmented_5x,\n",
    "              mixed_augmented]:\n",
    "        d.drop_duplicates(subset='source', inplace=True)\n",
    "    \n",
    "    # Shuffle datasets\n",
    "    augmented_2x_shuffled = augmented_2x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    augmented_5x_shuffled = augmented_5x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    augmented_10x_shuffled = augmented_10x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    augmented_20x_shuffled = augmented_20x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    augmented_50x_shuffled = augmented_50x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    \n",
    "    partial_augmented_5x_shuffled = partial_augmented_5x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    randomized_partial_augmented_5x_shuffled = randomized_partial_augmented_5x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    scrambled_partial_augmented_5x_shuffled = scrambled_partial_augmented_5x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    randomized_scrambled_partial_augmented_5x_shuffled = randomized_scrambled_partial_augmented_5x.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    mixed_augmented_shuffled = mixed_augmented.sample(frac=1, random_state=aug_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Export all datasets (filenames include the seed)\n",
    "    df[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'c1-{aug_seed}.csv'), index=False)\n",
    "    randomized[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'nc1-{aug_seed}.csv'), index=False)\n",
    "    scrambled_df[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'r1-{aug_seed}.csv'), index=False)\n",
    "    \n",
    "    augmented_2x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'a2-{aug_seed}.csv'), index=False)\n",
    "    augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'a5-{aug_seed}.csv'), index=False)\n",
    "    augmented_10x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'a10-{aug_seed}.csv'), index=False)\n",
    "    augmented_20x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'a20-{aug_seed}.csv'), index=False)\n",
    "    augmented_50x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'a50-{aug_seed}.csv'), index=False)\n",
    "    \n",
    "    partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'npstereo-{aug_seed}.csv'), index=False)\n",
    "    randomized_partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'ncnpstereo-{aug_seed}.csv'), index=False)\n",
    "    scrambled_partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'rp-{aug_seed}.csv'), index=False)\n",
    "    randomized_scrambled_partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'ncrp-{aug_seed}.csv'), index=False)\n",
    "    mixed_augmented_shuffled[['id', 'source', 'target', 'split']].to_csv(os.path.join(output_dir, f'm65-{aug_seed}.csv'), index=False)\n",
    "    \n",
    "    print(f\"Finished processing {input_file} with augmentation seed {aug_seed}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 1, 42]\n",
    "\n",
    "for seed_val in seeds:\n",
    "    input_file = f'data/coconut/coconut-split-{seed_val}.csv'\n",
    "    output_dir = f'data/augmented/seed-{seed_val}'\n",
    "    process_split_dataset(input_file, output_dir, aug_seed=seed_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiralpred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
