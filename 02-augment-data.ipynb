{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "from eval_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/coconut/coconut_split.csv')\n",
    "df.rename(columns={'identifier': 'id', 'smiles': 'target', 'split': 'split'}, inplace=True)\n",
    "\n",
    "df = df[['id', 'target', 'split']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canonicalize SMILES (they already are, however we want to keep the aromaticity information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate equivalent absolute SMILES. This step is done here to remove faulty SMILES from the dataset. The step is repeated later for the dataset augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = df['target'].apply(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(row):\n",
    "    target_mol = Chem.MolFromSmiles(row['target'])\n",
    "    source_mol = Chem.MolFromSmiles(row['source'])\n",
    "    target_smiles = Chem.MolToSmiles(target_mol, canonical=True, isomericSmiles=False)\n",
    "    source_smiles = Chem.MolToSmiles(source_mol, canonical=True, isomericSmiles=False)\n",
    "    return target_smiles == source_smiles\n",
    "\n",
    "checks = df.apply(sanity_check, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove annoying SMILES (where the two flat structures are not the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[checks].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dataset with scrambled stereocenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def exact_swap_stereochemistry(text):\n",
    "    text = text.replace('@@', 'TEMP_DOUBLE_AT')\n",
    "    text = text.replace('@', '@@')\n",
    "    text = text.replace('TEMP_DOUBLE_AT', '@')\n",
    "    text = text.replace('/', 'TEMP_SLASH').replace('\\\\', '/').replace('TEMP_SLASH', '\\\\')\n",
    "    return text\n",
    "\n",
    "def scramble_stereochemistry(text):\n",
    "    def random_replacement(match):\n",
    "        return random.choice([\"@\", \"@@\"])\n",
    "    def random_slash_backslash(match):\n",
    "        return random.choice([\"/\", \"\\\\\"])\n",
    "    text = re.sub(r'@@|@', random_replacement, text)\n",
    "    text = re.sub(r'/|\\\\', random_slash_backslash, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "scrambled_df = df.copy()\n",
    "scrambled_df['target'] = scrambled_df['target'].apply(scramble_stereochemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment data by SMILES randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare function to randomize SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_smiles(smiles, seed=None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    atoms = list(range(mol.GetNumAtoms()))\n",
    "    np.random.shuffle(atoms)\n",
    "    new_mol = Chem.RenumberAtoms(mol, atoms)\n",
    "    return Chem.MolToSmiles(new_mol, canonical=False, isomericSmiles=True)\n",
    "\n",
    "def randomize_augment_dataframe(df, factor):\n",
    "    augmented_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for i in range(factor):\n",
    "            new_smiles = randomize_smiles(row['target'], seed=i)\n",
    "            augmented_data.append({'id': row['id'], 'target': new_smiles, 'split': row['split']})\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions = {\n",
    "    r'\\[K[@,H]*\\]': '[K]',\n",
    "    r'\\[B[@,H]*\\]': 'B',\n",
    "    r'\\[Na[@,H,+,-]*\\]': '[Na]',\n",
    "    r'\\[C[@,H]*\\]': 'C',\n",
    "    r'\\[N[@,H]*\\]': 'N',\n",
    "    r'\\[O[@,H]*\\]': 'O',\n",
    "    r'\\[S[@,H]*\\]': 'S',\n",
    "    r'\\[P[@,H]*\\]': 'P',\n",
    "    r'\\[F[@,H]*\\]': 'F',\n",
    "    r'\\[Cl[@,H]*\\]': '[Cl]',\n",
    "    r'\\[Br[@,H]*\\]': '[Br]',\n",
    "    r'\\[I[@,H]*\\]': 'I',\n",
    "    r'@': '',\n",
    "    r'/': '',\n",
    "    r'\\\\': '',\n",
    "    r'\\[C\\]': 'C'\n",
    "}\n",
    "\n",
    "def apply_substitutions(smiles):\n",
    "    \"\"\"Apply predefined substitutions to the SMILES string.\"\"\"\n",
    "    for pattern, replacement in substitutions.items():\n",
    "        smiles = re.sub(pattern, replacement, smiles)\n",
    "    return smiles\n",
    "\n",
    "def generate_n_permutations(smiles, matches, num_to_remove, n, seed=None):\n",
    "    \"\"\"Generate exactly 'n' permutations where 'num_to_remove' matches are replaced with substitutions.\"\"\"\n",
    "    permutations = set()\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    if len(matches) < num_to_remove:\n",
    "        return list(permutations)\n",
    "\n",
    "    all_combinations = list(itertools.combinations(range(len(matches)), num_to_remove))\n",
    "    \n",
    "    if len(all_combinations) <= n:\n",
    "        selected_combinations = all_combinations\n",
    "    else:\n",
    "        selected_combinations = random.sample(all_combinations, n)\n",
    "\n",
    "    for indices in selected_combinations:\n",
    "        modified_smiles = list(smiles)\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            match_start = matches[index].start()\n",
    "            match_end = matches[index].end()\n",
    "            match_str = smiles[match_start:match_end]\n",
    "            modified_smiles[match_start:match_end] = apply_substitutions(match_str)\n",
    "\n",
    "        permutations.add(''.join(modified_smiles))\n",
    "\n",
    "    return list(permutations)\n",
    "\n",
    "def generate_random_permutations(smiles, matches, max_augmentations=50, seed=None, max_attempts=1000):\n",
    "    \"\"\"Generate a specified number of random permutations by replacing matches with substitutions.\"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    augmentations = set()\n",
    "    attempts = 0\n",
    "\n",
    "    while len(augmentations) < max_augmentations and attempts < max_attempts:\n",
    "        num_to_remove = random.randint(1, len(matches))\n",
    "        selected_indices = random.sample(range(len(matches)), num_to_remove)\n",
    "        modified_smiles = list(smiles)\n",
    "\n",
    "        for index in sorted(selected_indices, reverse=True):\n",
    "            match_start = matches[index].start()\n",
    "            match_end = matches[index].end()\n",
    "            match_str = smiles[match_start:match_end]\n",
    "            modified_smiles[match_start:match_end] = apply_substitutions(match_str)\n",
    "\n",
    "        augmented_smiles = ''.join(modified_smiles)  \n",
    "        augmentations.add(augmented_smiles)\n",
    "        \n",
    "        attempts += 1\n",
    "\n",
    "    return list(augmentations)\n",
    "\n",
    "def uniform_augment(smiles, n, seed=None):\n",
    "    \"\"\"Augment SMILES by generating 'n' augmentations for each number of matches replaced.\"\"\"\n",
    "    pattern = r'(\\[.*?\\]|[\\\\/])'\n",
    "    matches = list(re.finditer(pattern, smiles))\n",
    "\n",
    "    if not matches:\n",
    "        return [smiles]\n",
    "\n",
    "    augmentations = set()\n",
    "    \n",
    "    if len(matches) > 20:\n",
    "        augmentations.update(generate_random_permutations(smiles, matches, max_augmentations=50, seed=seed))\n",
    "    else:\n",
    "        for num_to_remove in range(1, len(matches) + 1):\n",
    "            augmentations.update(generate_n_permutations(smiles, matches, num_to_remove, n, seed))\n",
    "\n",
    "    return list(augmentations)\n",
    "\n",
    "def uniform_augment_dataframe(df, smiles_column, id_column, split_column, n=2, seed=None):\n",
    "    \"\"\"Augment SMILES in the specified column of the DataFrame while keeping id and split columns.\"\"\"\n",
    "    augmented_data = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        original_smiles = row[smiles_column]\n",
    "        augmented_smiles_list = uniform_augment(original_smiles, n, seed)\n",
    "\n",
    "        for augmented_smiles in augmented_smiles_list:\n",
    "            augmented_data.append({\n",
    "                id_column: row[id_column],\n",
    "                smiles_column: original_smiles,\n",
    "                'source': augmented_smiles,\n",
    "                split_column: row[split_column]\n",
    "            })\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets with different augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized = randomize_augment_dataframe(df, 1)\n",
    "scrambled_randomized = randomize_augment_dataframe(scrambled_df, 1)\n",
    "augmented_2x = randomize_augment_dataframe(df, 2)\n",
    "augmented_5x = randomize_augment_dataframe(df, 5)\n",
    "augmented_10x = randomize_augment_dataframe(df, 10)\n",
    "augmented_20x = randomize_augment_dataframe(df, 20)\n",
    "augmented_50x = randomize_augment_dataframe(df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate equivalent achiral SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized['source'] = randomized['target'].apply(flatten)\n",
    "scrambled_randomized['source'] = scrambled_randomized['target'].apply(flatten)\n",
    "augmented_2x['source'] = augmented_2x['target'].apply(flatten)\n",
    "augmented_5x['source'] = augmented_5x['target'].apply(flatten)\n",
    "augmented_10x['source'] = augmented_10x['target'].apply(flatten)\n",
    "augmented_20x['source'] = augmented_20x['target'].apply(flatten)\n",
    "augmented_50x['source'] = augmented_50x['target'].apply(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_augmented_5x = uniform_augment_dataframe(df, 'target', 'id', 'split', 5, seed=42)\n",
    "randomized_partial_augmented_5x = uniform_augment_dataframe(randomized, 'target', 'id', 'split', 5, seed=42)\n",
    "scrambled_partial_augmented_5x = uniform_augment_dataframe(scrambled_df, 'target', 'id', 'split', 5, seed=42)\n",
    "randomized_scrambled_partial_augmented_5x = uniform_augment_dataframe(scrambled_randomized, 'target', 'id', 'split', 5, seed=42)\n",
    "mixed_augmented = uniform_augment_dataframe(augmented_10x, 'target', 'id', 'split', 1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_2x.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_5x.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_10x.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_20x.drop_duplicates(subset='target', inplace=True)\n",
    "augmented_50x.drop_duplicates(subset='target', inplace=True)\n",
    "\n",
    "partial_augmented_5x.drop_duplicates(subset='source', inplace=True)\n",
    "randomized_partial_augmented_5x.drop_duplicates(subset='source', inplace=True)\n",
    "scrambled_partial_augmented_5x.drop_duplicates(subset='source', inplace=True)\n",
    "randomized_scrambled_partial_augmented_5x.drop_duplicates(subset='source', inplace=True)\n",
    "mixed_augmented.drop_duplicates(subset='source', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_2x_shuffled = augmented_2x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "augmented_5x_shuffled = augmented_5x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "augmented_10x_shuffled = augmented_10x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "augmented_20x_shuffled = augmented_20x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "augmented_50x_shuffled = augmented_50x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "partial_augmented_5x_shuffled = partial_augmented_5x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "randomized_partial_augmented_5x_shuffled = randomized_partial_augmented_5x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "scrambled_partial_augmented_5x_shuffled = scrambled_partial_augmented_5x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "randomized_scrambled_partial_augmented_5x_shuffled = randomized_scrambled_partial_augmented_5x.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "mixed_augmented_shuffled = mixed_augmented.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export augmented datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save augmented data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'source', 'target', 'split']].to_csv('data/dataset_not_augmented.csv', index=False)\n",
    "randomized[['id', 'source', 'target', 'split']].to_csv('data/dataset_randomized.csv', index=False)\n",
    "scrambled_df[['id', 'source', 'target', 'split']].to_csv('data/dataset_scrambled.csv', index=False)\n",
    "\n",
    "augmented_2x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_2x.csv', index=False)\n",
    "augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_5x.csv', index=False)\n",
    "augmented_10x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_10x.csv', index=False)\n",
    "augmented_20x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_20x.csv', index=False)\n",
    "augmented_50x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_augmented_50x.csv', index=False)\n",
    "\n",
    "partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_partial_augmented_5x.csv', index=False)\n",
    "randomized_partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_randomized_partial_augmented_5x.csv', index=False)\n",
    "scrambled_partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_scrambled_partial_augmented_5x.csv', index=False)\n",
    "randomized_scrambled_partial_augmented_5x_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_randomized_scrambled_partial_augmented_5x.csv', index=False)\n",
    "mixed_augmented_shuffled[['id', 'source', 'target', 'split']].to_csv('data/dataset_mixed_augmented.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiralpred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
